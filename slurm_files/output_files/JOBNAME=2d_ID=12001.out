Initialization time: 2025-06-16 21:28:45

**************************************************  Experiment 39 | Version 1 **************************************************
Label column '5y' found in metadata. Available columns: ['pid', 'study_yr', 'path', 'sct_slice_num_og', 'stage', '1y', '2y', '5y', '10y', 'fup_days', 'lung_side', 'de_stag', 'de_stag_7thed', 'sct_slice_new', 'sct_nod_err']

‚úÖ Saved split assignments to 'lung_metadata_with_splits.csv'

Using WeightedRandomSampler for train subset

Using WeightedRandomSampler for train subset

Using WeightedRandomSampler for train subset

‚úÖ Set train dataloaders with 3 folds
  - Fold 1: 210 samples
    Label distribution: {0: 158, 1: 52}
    Batch label distribution: {0.0: 8, 1.0: 8}
    Batch label distribution: {0.0: 12, 1.0: 4}
    Batch label distribution: {0.0: 6, 1.0: 10}
    Batch label distribution: {0.0: 9, 1.0: 7}
    Batch label distribution: {0.0: 7, 1.0: 9}
    Batch label distribution: {0.0: 7, 1.0: 9}
    Batch label distribution: {0.0: 4, 1.0: 12}
    Batch label distribution: {0.0: 9, 1.0: 7}
    Batch label distribution: {0.0: 6, 1.0: 10}
    Batch label distribution: {0.0: 10, 1.0: 6}
    Batch label distribution: {0.0: 7, 1.0: 9}
    Batch label distribution: {0.0: 6, 1.0: 10}
    Batch label distribution: {0.0: 12, 1.0: 4}
    Batch label distribution: {0.0: 1, 1.0: 1}
  - Fold 2: 210 samples
    Label distribution: {0: 158, 1: 52}
    Batch label distribution: {0.0: 5, 1.0: 11}
    Batch label distribution: {0.0: 7, 1.0: 9}
    Batch label distribution: {0.0: 9, 1.0: 7}
    Batch label distribution: {0.0: 6, 1.0: 10}
    Batch label distribution: {0.0: 7, 1.0: 9}
    Batch label distribution: {0.0: 7, 1.0: 9}
    Batch label distribution: {0.0: 10, 1.0: 6}
    Batch label distribution: {0.0: 9, 1.0: 7}
    Batch label distribution: {0.0: 7, 1.0: 9}
    Batch label distribution: {0.0: 9, 1.0: 7}
    Batch label distribution: {0.0: 11, 1.0: 5}
    Batch label distribution: {0.0: 8, 1.0: 8}
    Batch label distribution: {0.0: 6, 1.0: 10}
    Batch label distribution: {1.0: 2}
  - Fold 3: 210 samples
    Label distribution: {0: 158, 1: 52}
    Batch label distribution: {0.0: 8, 1.0: 8}
    Batch label distribution: {0.0: 7, 1.0: 9}
    Batch label distribution: {0.0: 10, 1.0: 6}
    Batch label distribution: {0.0: 8, 1.0: 8}
    Batch label distribution: {0.0: 9, 1.0: 7}
    Batch label distribution: {0.0: 8, 1.0: 8}
    Batch label distribution: {0.0: 10, 1.0: 6}
    Batch label distribution: {0.0: 8, 1.0: 8}
    Batch label distribution: {0.0: 11, 1.0: 5}
    Batch label distribution: {0.0: 7, 1.0: 9}
    Batch label distribution: {0.0: 10, 1.0: 6}
    Batch label distribution: {0.0: 13, 1.0: 3}
    Batch label distribution: {0.0: 8, 1.0: 8}
    Batch label distribution: {1.0: 2}

Using WeightedRandomSampler for validation subset

Using WeightedRandomSampler for validation subset

Using WeightedRandomSampler for validation subset

‚úÖ Set validation dataloaders with 3 folds
  - Fold 1: 105 samples
    Label distribution: {0: 79, 1: 26}
    Batch label distribution: {0.0: 13, 1.0: 3}
    Batch label distribution: {0.0: 12, 1.0: 4}
    Batch label distribution: {0.0: 11, 1.0: 5}
    Batch label distribution: {0.0: 13, 1.0: 3}
    Batch label distribution: {0.0: 12, 1.0: 4}
    Batch label distribution: {0.0: 12, 1.0: 4}
    Batch label distribution: {0.0: 6, 1.0: 3}
  - Fold 2: 105 samples
    Label distribution: {0: 79, 1: 26}
    Batch label distribution: {0.0: 12, 1.0: 4}
    Batch label distribution: {0.0: 14, 1.0: 2}
    Batch label distribution: {0.0: 14, 1.0: 2}
    Batch label distribution: {0.0: 12, 1.0: 4}
    Batch label distribution: {0.0: 11, 1.0: 5}
    Batch label distribution: {0.0: 11, 1.0: 5}
    Batch label distribution: {0.0: 5, 1.0: 4}
  - Fold 3: 105 samples
    Label distribution: {0: 79, 1: 26}
    Batch label distribution: {0.0: 10, 1.0: 6}
    Batch label distribution: {0.0: 12, 1.0: 4}
    Batch label distribution: {0.0: 13, 1.0: 3}
    Batch label distribution: {0.0: 13, 1.0: 3}
    Batch label distribution: {0.0: 10, 1.0: 6}
    Batch label distribution: {0.0: 13, 1.0: 3}
    Batch label distribution: {0.0: 8, 1.0: 1}

Using WeightedRandomSampler for test subset

Using WeightedRandomSampler for test subset

Using WeightedRandomSampler for test subset

‚úÖ Set test dataloaders with 3 folds
  - Fold 1: 79 samples
    Label distribution: {0: 59, 1: 20}
    Batch label distribution: {0.0: 12, 1.0: 4}
    Batch label distribution: {0.0: 13, 1.0: 3}
    Batch label distribution: {0.0: 11, 1.0: 5}
    Batch label distribution: {0.0: 13, 1.0: 3}
    Batch label distribution: {0.0: 10, 1.0: 5}
  - Fold 2: 79 samples
    Label distribution: {0: 59, 1: 20}
    Batch label distribution: {0.0: 12, 1.0: 4}
    Batch label distribution: {0.0: 13, 1.0: 3}
    Batch label distribution: {0.0: 11, 1.0: 5}
    Batch label distribution: {0.0: 13, 1.0: 3}
    Batch label distribution: {0.0: 10, 1.0: 5}
  - Fold 3: 79 samples
    Label distribution: {0: 59, 1: 20}
wandb: Currently logged in as: maria-i-paiva (maria-i-paiva-inesc-tec) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /nas-ctm01/homes/mipaiva/pipeline_template_pl/experiment_results/experiment_39/version_1/wandb/run-20250616_212907-jhzjoaf6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run resnet18_2d_39_1_fold1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/maria-i-paiva-inesc-tec/cs_2d_roi_whole_slice_oneclass_22_splits_wrs
wandb: üöÄ View run at https://wandb.ai/maria-i-paiva-inesc-tec/cs_2d_roi_whole_slice_oneclass_22_splits_wrs/runs/jhzjoaf6
    Batch label distribution: {0.0: 12, 1.0: 4}
    Batch label distribution: {0.0: 13, 1.0: 3}
    Batch label distribution: {0.0: 11, 1.0: 5}
    Batch label distribution: {0.0: 13, 1.0: 3}
    Batch label distribution: {0.0: 10, 1.0: 5}

Running model pipeline for data fold 1...
Label column '5y' found in metadata. Available columns: ['pid', 'study_yr', 'path', 'sct_slice_num_og', 'stage', '1y', '2y', '5y', '10y', 'fup_days', 'lung_side', 'de_stag', 'de_stag_7thed', 'sct_slice_new', 'sct_nod_err']
Label weights: tensor([3.0204], device='cuda:0', dtype=torch.float64)
srun: got SIGCONT
slurmstepd-02.ctm-deep-04: error: *** STEP 12001.0 ON 02.ctm-deep-04 CANCELLED AT 2025-06-16T21:31:33 ***
slurmstepd-02.ctm-deep-04: error: *** JOB 12001 ON 02.ctm-deep-04 CANCELLED AT 2025-06-16T21:31:33 ***
srun: forcing job termination
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
Error executing job with overrides: []
Traceback (most recent call last):
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py", line 43, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 571, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 980, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self.fit_loop.run()
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py", line 202, in run
    self.advance()
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py", line 355, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 219, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 188, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 266, in _optimizer_step
    call._call_lightning_module_hook(
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py", line 146, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/core/module.py", line 1276, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/core/optimizer.py", line 161, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py", line 231, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 116, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/torch/optim/optimizer.py", line 33, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/torch/optim/adam.py", line 121, in step
    loss = closure()
           ^^^^^^^^^
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 103, in _wrap_closure
    closure_result = closure()
                     ^^^^^^^^^
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 142, in __call__
    self._result = self.closure(*args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 137, in closure
    self._backward_fn(step_output.closure_loss)
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 237, in backward_fn
    call._call_strategy_hook(self.trainer, "backward", loss, optimizer)
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py", line 294, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py", line 205, in backward
    self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 69, in backward
    model.backward(tensor, *args, **kwargs)
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/core/module.py", line 1064, in backward
    loss.backward(*args, **kwargs)
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 139524) is killed by signal: Terminated. 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/nas-ctm01/homes/mipaiva/pipeline_template_pl/slurm_files/shell_script_files/../../src/scripts/run_experiment_pipeline.py", line 54, in run_hyperparameter_grid_based_execution_pipeline
    run_experiment_pipeline(config)
  File "/nas-ctm01/homes/mipaiva/pipeline_template_pl/slurm_files/shell_script_files/../../src/scripts/run_experiment_pipeline.py", line 167, in run_experiment_pipeline
    model_pipeline.train_model()
  File "/nas-ctm01/homes/mipaiva/pipeline_template_pl/src/modules/model/model_pipeline.py", line 68, in train_model
    self.pytorch_lightning_trainer.fit(
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 532, in fit
    call._call_and_handle_interrupt(
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py", line 67, in _call_and_handle_interrupt
    trainer._teardown()
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 1003, in _teardown
    self.strategy.teardown()
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py", line 494, in teardown
    _optimizers_to_device(self.optimizers, torch.device("cpu"))
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/lightning_fabric/utilities/optimizer.py", line 28, in _optimizers_to_device
    _optimizer_to_device(opt, device)
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/lightning_fabric/utilities/optimizer.py", line 34, in _optimizer_to_device
    optimizer.state[p] = apply_to_collection(v, Tensor, move_data_to_device, device, allow_frozen=True)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/lightning_utilities/core/apply_func.py", line 54, in apply_to_collection
    return _apply_to_collection_slow(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/lightning_utilities/core/apply_func.py", line 106, in _apply_to_collection_slow
    v = _apply_to_collection_slow(
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/lightning_utilities/core/apply_func.py", line 98, in _apply_to_collection_slow
    return function(data, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/lightning_fabric/utilities/apply_func.py", line 101, in move_data_to_device
    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/lightning_utilities/core/apply_func.py", line 66, in apply_to_collection
    return function(data, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/lightning_fabric/utilities/apply_func.py", line 95, in batch_to
    data_output = data.to(device, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 139744) is killed by signal: Terminated. 

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Exception ignored in atexit callback: <function _start_and_connect_service.<locals>.teardown_atexit at 0x74852f630a40>
Traceback (most recent call last):
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/wandb/sdk/lib/service_connection.py", line 94, in teardown_atexit
    conn.teardown(hooks.exit_code)
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/wandb/sdk/lib/service_connection.py", line 228, in teardown
    self._client.send_server_request(
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/wandb/sdk/lib/sock_client.py", line 154, in send_server_request
    self._send_message(msg)
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/wandb/sdk/lib/sock_client.py", line 151, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
           ^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe
srun: error: 02.ctm-deep-04: task 0: Exited with exit code 1
