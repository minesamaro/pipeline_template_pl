Initialization time: 2025-05-19 18:41:36
wandb: Currently logged in as: maria-i-paiva (maria-i-paiva-inesc-tec) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /nas-ctm01/homes/mipaiva/pipeline_template_pl/experiment_results/experiment_106/version_1/wandb/run-20250519_184158-305mbnkc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vgg16_3d_106_1_fold1
wandb: ⭐️ View project at https://wandb.ai/maria-i-paiva-inesc-tec/comparative_study_3d_norm
wandb: 🚀 View run at https://wandb.ai/maria-i-paiva-inesc-tec/comparative_study_3d_norm/runs/305mbnkc
/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 14 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
  warnings.warn(*args, **kwargs)
/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 14 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 14 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:  test_accuracy ▁▁▁
wandb:     test_auroc ▁▁▁
wandb: test_precision ▁▁▁
wandb:    test_recall ▁▁▁
wandb:    train_auroc █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     train_loss █▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁████████████████
wandb:      val_auroc ▁████████████████
wandb:       val_loss █▄▁▁▁▁▂▁▁▁▁▂▁▂▁▁▁
wandb:  val_precision ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     val_recall ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:  test_accuracy 0.75
wandb:     test_auroc 0.5
wandb: test_precision 0
wandb:    test_recall 0
wandb:    train_auroc 0.5
wandb:     train_loss 0.3812
wandb:   val_accuracy 0.75
wandb:      val_auroc 0.5
wandb:       val_loss 0.37518
wandb:  val_precision 0
wandb:     val_recall 0
wandb: 
wandb: 🚀 View run vgg16_3d_106_1_fold1 at: https://wandb.ai/maria-i-paiva-inesc-tec/comparative_study_3d_norm/runs/305mbnkc
wandb: ⭐️ View project at: https://wandb.ai/maria-i-paiva-inesc-tec/comparative_study_3d_norm
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /nas-ctm01/homes/mipaiva/pipeline_template_pl/experiment_results/experiment_106/version_1/wandb/run-20250519_184158-305mbnkc/logs
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /nas-ctm01/homes/mipaiva/pipeline_template_pl/experiment_results/experiment_106/version_2/wandb/run-20250519_191242-keovl5cg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vgg16_3d_106_2_fold1
wandb: ⭐️ View project at https://wandb.ai/maria-i-paiva-inesc-tec/comparative_study_3d_norm
wandb: 🚀 View run at https://wandb.ai/maria-i-paiva-inesc-tec/comparative_study_3d_norm/runs/keovl5cg
/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 14 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 14 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 14 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:  test_accuracy ▁▁▁
wandb:     test_auroc ▁▁▁
wandb: test_precision ▁▁▁
wandb:    test_recall ▁▁▁
wandb:    train_auroc █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁████████████████
wandb:      val_auroc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       val_loss █▂▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁
wandb:  val_precision █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     val_recall █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:  test_accuracy 0.75
wandb:     test_auroc 0.5
wandb: test_precision 0
wandb:    test_recall 0
wandb:    train_auroc 0.5
wandb:     train_loss 0.37853
wandb:   val_accuracy 0.75
wandb:      val_auroc 0.5
wandb:       val_loss 0.37558
wandb:  val_precision 0
wandb:     val_recall 0
wandb: 
wandb: 🚀 View run vgg16_3d_106_2_fold1 at: https://wandb.ai/maria-i-paiva-inesc-tec/comparative_study_3d_norm/runs/keovl5cg
wandb: ⭐️ View project at: https://wandb.ai/maria-i-paiva-inesc-tec/comparative_study_3d_norm
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /nas-ctm01/homes/mipaiva/pipeline_template_pl/experiment_results/experiment_106/version_2/wandb/run-20250519_191242-keovl5cg/logs
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /nas-ctm01/homes/mipaiva/pipeline_template_pl/experiment_results/experiment_106/version_3/wandb/run-20250519_202742-hb510us3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vgg16_3d_106_3_fold1
wandb: ⭐️ View project at https://wandb.ai/maria-i-paiva-inesc-tec/comparative_study_3d_norm
wandb: 🚀 View run at https://wandb.ai/maria-i-paiva-inesc-tec/comparative_study_3d_norm/runs/hb510us3
/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 14 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 14 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 14 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:  test_accuracy ▁▁▁
wandb:     test_auroc ▁▁▁
wandb: test_precision ▁▁▁
wandb:    test_recall ▁▁▁
wandb:    train_auroc █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁████████████████
wandb:      val_auroc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       val_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  val_precision █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     val_recall █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:  test_accuracy 0.75
wandb:     test_auroc 0.5
wandb: test_precision 0
wandb:    test_recall 0
wandb:    train_auroc 0.5
wandb:     train_loss 0.37945
wandb:   val_accuracy 0.75
wandb:      val_auroc 0.5
wandb:       val_loss 0.37543
wandb:  val_precision 0
wandb:     val_recall 0
wandb: 
wandb: 🚀 View run vgg16_3d_106_3_fold1 at: https://wandb.ai/maria-i-paiva-inesc-tec/comparative_study_3d_norm/runs/hb510us3
wandb: ⭐️ View project at: https://wandb.ai/maria-i-paiva-inesc-tec/comparative_study_3d_norm
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /nas-ctm01/homes/mipaiva/pipeline_template_pl/experiment_results/experiment_106/version_3/wandb/run-20250519_202742-hb510us3/logs
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /nas-ctm01/homes/mipaiva/pipeline_template_pl/experiment_results/experiment_106/version_4/wandb/run-20250519_214253-9szfaw6d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vgg16_3d_106_4_fold1
wandb: ⭐️ View project at https://wandb.ai/maria-i-paiva-inesc-tec/comparative_study_3d_norm
wandb: 🚀 View run at https://wandb.ai/maria-i-paiva-inesc-tec/comparative_study_3d_norm/runs/9szfaw6d
/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 14 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
  warnings.warn(*args, **kwargs)
/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 14 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 14 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:  test_accuracy ▁▁▁
wandb:     test_auroc ▁▁▁
wandb: test_precision ▁▁▁
wandb:    test_recall ▁▁▁
wandb:    train_auroc ▁███████████████
wandb:     train_loss █▅▁▂▂▃▃▃▁▂▂▂▁▁▃▁
wandb:   val_accuracy ▁████████████████
wandb:      val_auroc ▁████████████████
wandb:       val_loss █▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  val_precision ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     val_recall ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:  test_accuracy 0.75
wandb:     test_auroc 0.5
wandb: test_precision 0
wandb:    test_recall 0
wandb:    train_auroc 0.5
wandb:     train_loss 0.37659
wandb:   val_accuracy 0.75
wandb:      val_auroc 0.5
wandb:       val_loss 0.37499
wandb:  val_precision 0
wandb:     val_recall 0
wandb: 
wandb: 🚀 View run vgg16_3d_106_4_fold1 at: https://wandb.ai/maria-i-paiva-inesc-tec/comparative_study_3d_norm/runs/9szfaw6d
wandb: ⭐️ View project at: https://wandb.ai/maria-i-paiva-inesc-tec/comparative_study_3d_norm
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /nas-ctm01/homes/mipaiva/pipeline_template_pl/experiment_results/experiment_106/version_4/wandb/run-20250519_214253-9szfaw6d/logs
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /nas-ctm01/homes/mipaiva/pipeline_template_pl/experiment_results/experiment_106/version_5/wandb/run-20250519_221033-g97pjf0z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vgg16_3d_106_5_fold1
wandb: ⭐️ View project at https://wandb.ai/maria-i-paiva-inesc-tec/comparative_study_3d_norm
wandb: 🚀 View run at https://wandb.ai/maria-i-paiva-inesc-tec/comparative_study_3d_norm/runs/g97pjf0z
/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 14 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 14 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 14 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:  test_accuracy ▁▁▁
wandb:     test_auroc ▁▁▁
wandb: test_precision ▁▁▁
wandb:    test_recall ▁▁▁
wandb:    train_auroc █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     train_loss █▄▁▂▂▂▁▁▂▂▁▁▂▂▁▁
wandb:   val_accuracy ▁████████████████
wandb:      val_auroc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       val_loss █▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  val_precision █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     val_recall █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:  test_accuracy 0.75
wandb:     test_auroc 0.5
wandb: test_precision 0
wandb:    test_recall 0
wandb:    train_auroc 0.5
wandb:     train_loss 0.37704
wandb:   val_accuracy 0.75
wandb:      val_auroc 0.5
wandb:       val_loss 0.37501
wandb:  val_precision 0
wandb:     val_recall 0
wandb: 
wandb: 🚀 View run vgg16_3d_106_5_fold1 at: https://wandb.ai/maria-i-paiva-inesc-tec/comparative_study_3d_norm/runs/g97pjf0z
wandb: ⭐️ View project at: https://wandb.ai/maria-i-paiva-inesc-tec/comparative_study_3d_norm
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /nas-ctm01/homes/mipaiva/pipeline_template_pl/experiment_results/experiment_106/version_5/wandb/run-20250519_221033-g97pjf0z/logs
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /nas-ctm01/homes/mipaiva/pipeline_template_pl/experiment_results/experiment_106/version_6/wandb/run-20250519_232605-070isce4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vgg16_3d_106_6_fold1
wandb: ⭐️ View project at https://wandb.ai/maria-i-paiva-inesc-tec/comparative_study_3d_norm
wandb: 🚀 View run at https://wandb.ai/maria-i-paiva-inesc-tec/comparative_study_3d_norm/runs/070isce4
/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 14 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 14 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 14 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:  test_accuracy ▁▁▁
wandb:     test_auroc ▁▁▁
wandb: test_precision ▁▁▁
wandb:    test_recall ▁▁▁
wandb:    train_auroc █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     train_loss █▇▂▁▁▂▁▂▁▁▁▁▁▁▁▁
wandb:   val_accuracy ▁████████████████
wandb:      val_auroc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       val_loss █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  val_precision █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     val_recall █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:  test_accuracy 0.75
wandb:     test_auroc 0.5
wandb: test_precision 0
wandb:    test_recall 0
wandb:    train_auroc 0.5
wandb:     train_loss 0.3767
wandb:   val_accuracy 0.75
wandb:      val_auroc 0.5
wandb:       val_loss 0.3749
wandb:  val_precision 0
wandb:     val_recall 0
wandb: 
wandb: 🚀 View run vgg16_3d_106_6_fold1 at: https://wandb.ai/maria-i-paiva-inesc-tec/comparative_study_3d_norm/runs/070isce4
wandb: ⭐️ View project at: https://wandb.ai/maria-i-paiva-inesc-tec/comparative_study_3d_norm
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /nas-ctm01/homes/mipaiva/pipeline_template_pl/experiment_results/experiment_106/version_6/wandb/run-20250519_232605-070isce4/logs
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /nas-ctm01/homes/mipaiva/pipeline_template_pl/experiment_results/experiment_106/version_7/wandb/run-20250520_004226-kn5qi6ad
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vgg16_3d_106_7_fold1
wandb: ⭐️ View project at https://wandb.ai/maria-i-paiva-inesc-tec/comparative_study_3d_norm
wandb: 🚀 View run at https://wandb.ai/maria-i-paiva-inesc-tec/comparative_study_3d_norm/runs/kn5qi6ad
/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 14 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
  warnings.warn(*args, **kwargs)
/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 14 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 14 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:  test_accuracy ▁▁▁
wandb:     test_auroc ▁▁▁
wandb: test_precision ▁▁▁
wandb:    test_recall ▁▁▁
wandb:    train_auroc █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     train_loss █▂▁▂▁▃▂▃▂▃▂▂▁▂▁▁
wandb:   val_accuracy ▁████████████████
wandb:      val_auroc ▁████████████████
wandb:       val_loss █▂▂▁▁▁▂▁▂▁▃▂▂▂▂▁▁
wandb:  val_precision ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     val_recall ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:  test_accuracy 0.75
wandb:     test_auroc 0.5
wandb: test_precision 0
wandb:    test_recall 0
wandb:    train_auroc 0.5
wandb:     train_loss 0.38112
wandb:   val_accuracy 0.75
wandb:      val_auroc 0.5
wandb:       val_loss 0.38183
wandb:  val_precision 0
wandb:     val_recall 0
wandb: 
wandb: 🚀 View run vgg16_3d_106_7_fold1 at: https://wandb.ai/maria-i-paiva-inesc-tec/comparative_study_3d_norm/runs/kn5qi6ad
wandb: ⭐️ View project at: https://wandb.ai/maria-i-paiva-inesc-tec/comparative_study_3d_norm
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /nas-ctm01/homes/mipaiva/pipeline_template_pl/experiment_results/experiment_106/version_7/wandb/run-20250520_004226-kn5qi6ad/logs
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /nas-ctm01/homes/mipaiva/pipeline_template_pl/experiment_results/experiment_106/version_8/wandb/run-20250520_011148-mbijygxp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vgg16_3d_106_8_fold1
wandb: ⭐️ View project at https://wandb.ai/maria-i-paiva-inesc-tec/comparative_study_3d_norm
wandb: 🚀 View run at https://wandb.ai/maria-i-paiva-inesc-tec/comparative_study_3d_norm/runs/mbijygxp
/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 14 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 14 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 14 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:  test_accuracy ▁▁▁
wandb:     test_auroc ▁▁▁
wandb: test_precision ▁▁▁
wandb:    test_recall ▁▁▁
wandb:    train_auroc █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     train_loss █▃▂▂▁▃▂▂▂▁▂▁▁▃▂▂
wandb:   val_accuracy ▁████████████████
wandb:      val_auroc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       val_loss █▅▁▂▁▁▁▁▁▁▂▃▁▃▂▂▂
wandb:  val_precision █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     val_recall █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:  test_accuracy 0.75
wandb:     test_auroc 0.5
wandb: test_precision 0
wandb:    test_recall 0
wandb:    train_auroc 0.5
wandb:     train_loss 0.3852
wandb:   val_accuracy 0.75
wandb:      val_auroc 0.5
wandb:       val_loss 0.38964
wandb:  val_precision 0
wandb:     val_recall 0
wandb: 
wandb: 🚀 View run vgg16_3d_106_8_fold1 at: https://wandb.ai/maria-i-paiva-inesc-tec/comparative_study_3d_norm/runs/mbijygxp
wandb: ⭐️ View project at: https://wandb.ai/maria-i-paiva-inesc-tec/comparative_study_3d_norm
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /nas-ctm01/homes/mipaiva/pipeline_template_pl/experiment_results/experiment_106/version_8/wandb/run-20250520_011148-mbijygxp/logs
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /nas-ctm01/homes/mipaiva/pipeline_template_pl/experiment_results/experiment_106/version_9/wandb/run-20250520_022707-ra87f5o7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vgg16_3d_106_9_fold1
wandb: ⭐️ View project at https://wandb.ai/maria-i-paiva-inesc-tec/comparative_study_3d_norm
wandb: 🚀 View run at https://wandb.ai/maria-i-paiva-inesc-tec/comparative_study_3d_norm/runs/ra87f5o7
/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 14 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 14 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/nas-ctm01/homes/mipaiva/.conda/envs/pipeline/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 14 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:  test_accuracy ▁▁▁
wandb:     test_auroc ▁▁▁
wandb: test_precision ▁▁▁
wandb:    test_recall ▁▁▁
wandb:    train_auroc █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     train_loss █▄▁▁▂▁▂▁▂▁▂▂▁▂▁▂
wandb:   val_accuracy ▁████████████████
wandb:      val_auroc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       val_loss █▇▁▂▁▁▁▂▁▁▁▃▂▂▁▁▁
wandb:  val_precision █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     val_recall █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:  test_accuracy 0.75
wandb:     test_auroc 0.5
wandb: test_precision 0
wandb:    test_recall 0
wandb:    train_auroc 0.5
wandb:     train_loss 0.38647
wandb:   val_accuracy 0.75
wandb:      val_auroc 0.5
wandb:       val_loss 0.37747
wandb:  val_precision 0
wandb:     val_recall 0
wandb: 
wandb: 🚀 View run vgg16_3d_106_9_fold1 at: https://wandb.ai/maria-i-paiva-inesc-tec/comparative_study_3d_norm/runs/ra87f5o7
wandb: ⭐️ View project at: https://wandb.ai/maria-i-paiva-inesc-tec/comparative_study_3d_norm
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /nas-ctm01/homes/mipaiva/pipeline_template_pl/experiment_results/experiment_106/version_9/wandb/run-20250520_022707-ra87f5o7/logs

**************************************************  Experiment 106 | Version 1 **************************************************
Label column '5y' found in metadata. Available columns: ['pid', 'study_yr', 'path', 'sct_slice_num_og', 'stage', '1y', '2y', '5y', '10y', 'fup_days', 'sct_nod_err']

Running model pipeline for data fold 1...
Label column '5y' found in metadata. Available columns: ['pid', 'study_yr', 'path', 'sct_slice_num_og', 'stage', '1y', '2y', '5y', '10y', 'fup_days', 'sct_nod_err']
...model pipeline for data fold 1 has run!
Saving execution datetimes to /nas-ctm01/homes/mipaiva/pipeline_template_pl/experiment_results/experiment_106/version_1/execution_datetimes.json

**************************************************  Experiment 106 | Version 2 **************************************************
Label column '5y' found in metadata. Available columns: ['pid', 'study_yr', 'path', 'sct_slice_num_og', 'stage', '1y', '2y', '5y', '10y', 'fup_days', 'sct_nod_err']

Running model pipeline for data fold 1...
Label column '5y' found in metadata. Available columns: ['pid', 'study_yr', 'path', 'sct_slice_num_og', 'stage', '1y', '2y', '5y', '10y', 'fup_days', 'sct_nod_err']
...model pipeline for data fold 1 has run!
Saving execution datetimes to /nas-ctm01/homes/mipaiva/pipeline_template_pl/experiment_results/experiment_106/version_2/execution_datetimes.json

**************************************************  Experiment 106 | Version 3 **************************************************
Label column '5y' found in metadata. Available columns: ['pid', 'study_yr', 'path', 'sct_slice_num_og', 'stage', '1y', '2y', '5y', '10y', 'fup_days', 'sct_nod_err']

Running model pipeline for data fold 1...
Label column '5y' found in metadata. Available columns: ['pid', 'study_yr', 'path', 'sct_slice_num_og', 'stage', '1y', '2y', '5y', '10y', 'fup_days', 'sct_nod_err']
...model pipeline for data fold 1 has run!
Saving execution datetimes to /nas-ctm01/homes/mipaiva/pipeline_template_pl/experiment_results/experiment_106/version_3/execution_datetimes.json

**************************************************  Experiment 106 | Version 4 **************************************************
Label column '5y' found in metadata. Available columns: ['pid', 'study_yr', 'path', 'sct_slice_num_og', 'stage', '1y', '2y', '5y', '10y', 'fup_days', 'sct_nod_err']

Running model pipeline for data fold 1...
Label column '5y' found in metadata. Available columns: ['pid', 'study_yr', 'path', 'sct_slice_num_og', 'stage', '1y', '2y', '5y', '10y', 'fup_days', 'sct_nod_err']
...model pipeline for data fold 1 has run!
Saving execution datetimes to /nas-ctm01/homes/mipaiva/pipeline_template_pl/experiment_results/experiment_106/version_4/execution_datetimes.json

**************************************************  Experiment 106 | Version 5 **************************************************
Label column '5y' found in metadata. Available columns: ['pid', 'study_yr', 'path', 'sct_slice_num_og', 'stage', '1y', '2y', '5y', '10y', 'fup_days', 'sct_nod_err']

Running model pipeline for data fold 1...
Label column '5y' found in metadata. Available columns: ['pid', 'study_yr', 'path', 'sct_slice_num_og', 'stage', '1y', '2y', '5y', '10y', 'fup_days', 'sct_nod_err']
...model pipeline for data fold 1 has run!
Saving execution datetimes to /nas-ctm01/homes/mipaiva/pipeline_template_pl/experiment_results/experiment_106/version_5/execution_datetimes.json

**************************************************  Experiment 106 | Version 6 **************************************************
Label column '5y' found in metadata. Available columns: ['pid', 'study_yr', 'path', 'sct_slice_num_og', 'stage', '1y', '2y', '5y', '10y', 'fup_days', 'sct_nod_err']

Running model pipeline for data fold 1...
Label column '5y' found in metadata. Available columns: ['pid', 'study_yr', 'path', 'sct_slice_num_og', 'stage', '1y', '2y', '5y', '10y', 'fup_days', 'sct_nod_err']
...model pipeline for data fold 1 has run!
Saving execution datetimes to /nas-ctm01/homes/mipaiva/pipeline_template_pl/experiment_results/experiment_106/version_6/execution_datetimes.json

**************************************************  Experiment 106 | Version 7 **************************************************
Label column '5y' found in metadata. Available columns: ['pid', 'study_yr', 'path', 'sct_slice_num_og', 'stage', '1y', '2y', '5y', '10y', 'fup_days', 'sct_nod_err']

Running model pipeline for data fold 1...
Label column '5y' found in metadata. Available columns: ['pid', 'study_yr', 'path', 'sct_slice_num_og', 'stage', '1y', '2y', '5y', '10y', 'fup_days', 'sct_nod_err']
...model pipeline for data fold 1 has run!
Saving execution datetimes to /nas-ctm01/homes/mipaiva/pipeline_template_pl/experiment_results/experiment_106/version_7/execution_datetimes.json

**************************************************  Experiment 106 | Version 8 **************************************************
Label column '5y' found in metadata. Available columns: ['pid', 'study_yr', 'path', 'sct_slice_num_og', 'stage', '1y', '2y', '5y', '10y', 'fup_days', 'sct_nod_err']

Running model pipeline for data fold 1...
Label column '5y' found in metadata. Available columns: ['pid', 'study_yr', 'path', 'sct_slice_num_og', 'stage', '1y', '2y', '5y', '10y', 'fup_days', 'sct_nod_err']
...model pipeline for data fold 1 has run!
Saving execution datetimes to /nas-ctm01/homes/mipaiva/pipeline_template_pl/experiment_results/experiment_106/version_8/execution_datetimes.json

**************************************************  Experiment 106 | Version 9 **************************************************
Label column '5y' found in metadata. Available columns: ['pid', 'study_yr', 'path', 'sct_slice_num_og', 'stage', '1y', '2y', '5y', '10y', 'fup_days', 'sct_nod_err']

Running model pipeline for data fold 1...
Label column '5y' found in metadata. Available columns: ['pid', 'study_yr', 'path', 'sct_slice_num_og', 'stage', '1y', '2y', '5y', '10y', 'fup_days', 'sct_nod_err']
...model pipeline for data fold 1 has run!
Saving execution datetimes to /nas-ctm01/homes/mipaiva/pipeline_template_pl/experiment_results/experiment_106/version_9/execution_datetimes.json
Finish time: 2025-05-20 03:42:46
